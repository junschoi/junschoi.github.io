[{"categories":["Python"],"contents":"The English Premier League is arguably the most difficult football league in the world. With the right use of tactics and players, any team can create upsets in the Premier League. Teams are always trying to recruit the next big star, the hidden gems, or just anyone who can successfully fill a role with the least amount of resources.\nIn this demo, I will use K-Means clustering to group players based on their shot and goal creating performance metrics. Hopefully this data-led approach will remove some biases when analyzing players and help identify players who may be a good fit for a specific team style.\n# Importing Libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.cluster import KMeans from sklearn.preprocessing import StandardScaler from sklearn.metrics import silhouette_score # from sklearn.model_selection import GridSearchCV # Loading data # Source: https://fbref.com/en/comps/9/gca/Premier-League-Stats df = pd.read_csv(\u0026#39;20210302_sca.csv\u0026#39;) df.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Rk Player Nation Pos Squad Age Born 90s SCA SCA90 ... GCA GCA90 PassLive.1 PassDead.1 Drib.1 Sh.1 Fld.1 Def.1 OG Matches     0 1 Patrick van Aanholt\\Patrick-van-Aanholt nl NED DF Crystal Palace 30-185 1990 13.7 22 1.60 ... 2 0.15 2 0 0 0 0 0 0 Matches   1 2 Tammy Abraham\\Tammy-Abraham eng ENG FW Chelsea 23-151 1997 11.3 18 1.59 ... 2 0.18 1 0 0 0 1 0 0 Matches   2 3 Che Adams\\Che-Adams eng ENG FW Southampton 24-232 1996 21.2 42 1.98 ... 6 0.28 5 0 0 0 1 0 0 Matches   3 4 Tosin Adarabioyo\\Tosin-Adarabioyo eng ENG DF Fulham 23-159 1997 22.0 12 0.55 ... 1 0.05 1 0 0 0 0 0 0 Matches   4 5 Adrián\\Adrian es ESP GK Liverpool 34-058 1987 3.0 0 0.00 ... 0 0.00 0 0 0 0 0 0 0 Matches    5 rows × 26 columns\n Data Cleaning Process # List of columns df.columns Index(['Rk', 'Player', 'Nation', 'Pos', 'Squad', 'Age', 'Born', '90s', 'SCA', 'SCA90', 'PassLive', 'PassDead', 'Drib', 'Sh', 'Fld', 'Def', 'GCA', 'GCA90', 'PassLive.1', 'PassDead.1', 'Drib.1', 'Sh.1', 'Fld.1', 'Def.1', 'OG', 'Matches'], dtype='object')  # New column names columns = [ \u0026#39;Rk\u0026#39;, \u0026#39;Player\u0026#39;, \u0026#39;Nation\u0026#39;, \u0026#39;Pos\u0026#39;, \u0026#39;Squad\u0026#39;, \u0026#39;Age\u0026#39;, \u0026#39;Born\u0026#39;, \u0026#39;90s\u0026#39;, \u0026#39;SCA\u0026#39;, \u0026#39;SCA90\u0026#39;, \u0026#39;SCA PassLive\u0026#39;, \u0026#39;SCA PassDead\u0026#39;, \u0026#39;SCA Drib\u0026#39;, \u0026#39;SCA Sh\u0026#39;, \u0026#39;SCA Fld\u0026#39;, \u0026#39;SCA Def\u0026#39;, \u0026#39;GCA\u0026#39;, \u0026#39;GCA90\u0026#39;, \u0026#39;GCA PassLive\u0026#39;, \u0026#39;GCA PassDead\u0026#39;, \u0026#39;GCA Drib\u0026#39;, \u0026#39;GCA Sh\u0026#39;, \u0026#39;GCA Fld\u0026#39;, \u0026#39;GCA Def\u0026#39;, \u0026#39;GCA OG\u0026#39;, \u0026#39;Matches\u0026#39; ] # Renaming columns df.columns = columns # Checking dtypes and non-null counts df.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 505 entries, 0 to 504 Data columns (total 26 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Rk 505 non-null int64 1 Player 505 non-null object 2 Nation 505 non-null object 3 Pos 505 non-null object 4 Squad 505 non-null object 5 Age 505 non-null object 6 Born 505 non-null int64 7 90s 505 non-null float64 8 SCA 505 non-null int64 9 SCA90 505 non-null float64 10 SCA PassLive 505 non-null int64 11 SCA PassDead 505 non-null int64 12 SCA Drib 505 non-null int64 13 SCA Sh 505 non-null int64 14 SCA Fld 505 non-null int64 15 SCA Def 505 non-null int64 16 GCA 505 non-null int64 17 GCA90 505 non-null float64 18 GCA PassLive 505 non-null int64 19 GCA PassDead 505 non-null int64 20 GCA Drib 505 non-null int64 21 GCA Sh 505 non-null int64 22 GCA Fld 505 non-null int64 23 GCA Def 505 non-null int64 24 GCA OG 505 non-null int64 25 Matches 505 non-null object dtypes: float64(3), int64(17), object(6) memory usage: 102.7+ KB  # Cleaning column values df[\u0026#39;Player\u0026#39;] = df[\u0026#39;Player\u0026#39;].str.split(\u0026#39;\\\\\u0026#39;, expand=True)[0] df[\u0026#39;Nation\u0026#39;] = df[\u0026#39;Nation\u0026#39;].str.split(\u0026#39; \u0026#39;, expand=True)[1] df[\u0026#39;Age\u0026#39;] = df[\u0026#39;Age\u0026#39;].str.split(\u0026#39;-\u0026#39;, expand=True)[0] # Dropping unused column df = df.drop(columns=[\u0026#39;Matches\u0026#39;]) df.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Rk Player Nation Pos Squad Age Born 90s SCA SCA90 ... SCA Def GCA GCA90 GCA PassLive GCA PassDead GCA Drib GCA Sh GCA Fld GCA Def GCA OG     0 1 Patrick van Aanholt NED DF Crystal Palace 30 1990 13.7 22 1.60 ... 0 2 0.15 2 0 0 0 0 0 0   1 2 Tammy Abraham ENG FW Chelsea 23 1997 11.3 18 1.59 ... 0 2 0.18 1 0 0 0 1 0 0   2 3 Che Adams ENG FW Southampton 24 1996 21.2 42 1.98 ... 0 6 0.28 5 0 0 0 1 0 0   3 4 Tosin Adarabioyo ENG DF Fulham 23 1997 22.0 12 0.55 ... 0 1 0.05 1 0 0 0 0 0 0   4 5 Adrián ESP GK Liverpool 34 1987 3.0 0 0.00 ... 0 0 0.00 0 0 0 0 0 0 0    5 rows × 25 columns\n Feature Engineering # List of SCA columns sca_cols = [\u0026#39;SCA PassLive\u0026#39;, \u0026#39;SCA PassDead\u0026#39;, \u0026#39;SCA Drib\u0026#39;, \u0026#39;SCA Sh\u0026#39;, \u0026#39;SCA Fld\u0026#39;, \u0026#39;SCA Def\u0026#39;] # List of new SCA column names new_sca_cols = [col + \u0026#39; Ratio\u0026#39; for col in sca_cols] # Adding SCA Ratios for old, new in zip(sca_cols, new_sca_cols): df[new] = df[old]/df[\u0026#39;SCA\u0026#39;] df[new] = df[new].fillna(0) # List of GCA columns gca_cols = [\u0026#39;GCA PassLive\u0026#39;, \u0026#39;GCA PassDead\u0026#39;, \u0026#39;GCA Drib\u0026#39;, \u0026#39;GCA Sh\u0026#39;, \u0026#39;GCA Fld\u0026#39;, \u0026#39;GCA Def\u0026#39;, \u0026#39;GCA OG\u0026#39;] # List of new GCA column names new_gca_cols = [col + \u0026#39; Ratio\u0026#39; for col in gca_cols] # Adding GCA Ratios for old, new in zip(gca_cols, new_gca_cols): df[new] = df[old]/df[\u0026#39;GCA\u0026#39;] df[new] = df[new].fillna(0) # List of new GCA/SCA column names new_sca_gca_cols = [ \u0026#39;GCA/SCA PassLive\u0026#39;, \u0026#39;GCA/SCA PassDead\u0026#39;, \u0026#39;GCA/SCA Drib\u0026#39;, \u0026#39;GCA/SCA Sh\u0026#39;, \u0026#39;GCA/SCA Fld\u0026#39;, \u0026#39;GCA/SCA Def\u0026#39; ] # Adding GCA/SCA Ratios for i in range(0, 6): df[new_sca_gca_cols[i]] = df[gca_cols[i]]/df[sca_cols[i]] df[new_sca_gca_cols[i]] = df[new_sca_gca_cols[i]].fillna(0) # Adding overall GCA/SCA Ratio df[\u0026#39;GCA/SCA\u0026#39;] = df[\u0026#39;GCA\u0026#39;]/df[\u0026#39;SCA\u0026#39;] df[\u0026#39;GCA/SCA\u0026#39;] = df[\u0026#39;GCA/SCA\u0026#39;].fillna(0) # Only including forwards and midfielders df = df.loc[df[\u0026#39;Pos\u0026#39;].isin([\u0026#39;FW\u0026#39;, \u0026#39;FWDF\u0026#39;, \u0026#39;MF\u0026#39;, \u0026#39;MFFW\u0026#39;, \u0026#39;MFDF\u0026#39;, \u0026#39;FWMF\u0026#39;])] K-Means Clustering drop_cols = [ \u0026#39;Rk\u0026#39;, \u0026#39;Player\u0026#39;, \u0026#39;Nation\u0026#39;, \u0026#39;Pos\u0026#39;, \u0026#39;Squad\u0026#39;, \u0026#39;Age\u0026#39;, \u0026#39;Born\u0026#39;, \u0026#39;90s\u0026#39;, \u0026#39;SCA\u0026#39;, \u0026#39;SCA90\u0026#39;, \u0026#39;GCA\u0026#39;, \u0026#39;GCA90\u0026#39; ] # Initializing KMeans object from sklearn km = KMeans(n_clusters=5, init=\u0026#39;k-means++\u0026#39;) # Fitting and predicting pred = km.fit_predict(df.drop(columns=drop_cols)) # Viewing predictions pred array([4, 2, 0, 2, 2, 4, 0, 2, 4, 0, 0, 4, 2, 2, 0, 4, 0, 2, 0, 2, 2, 1, 4, 0, 2, 0, 4, 0, 4, 4, 4, 2, 2, 4, 0, 4, 4, 4, 4, 0, 0, 2, 4, 2, 0, 0, 0, 4, 0, 4, 0, 3, 0, 0, 2, 0, 0, 4, 1, 4, 2, 0, 4, 2, 3, 4, 0, 1, 2, 2, 2, 4, 2, 4, 0, 4, 0, 0, 4, 4, 0, 0, 0, 3, 4, 1, 0, 1, 4, 0, 1, 2, 0, 2, 4, 4, 1, 0, 1, 2, 0, 0, 2, 2, 4, 0, 0, 2, 4, 4, 2, 2, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 3, 2, 4, 4, 2, 4, 4, 0, 4, 0, 4, 0, 2, 0, 0, 0, 1, 2, 2, 4, 1, 1, 0, 0, 1, 2, 0, 4, 0, 4, 2, 4, 4, 0, 1, 4, 2, 4, 0, 4, 4, 0, 0, 2, 0, 0, 1, 4, 0, 2, 0, 0, 0, 2, 0, 1, 2, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 4, 4, 2, 2, 2, 4, 2, 2, 4, 0, 2, 0, 1, 4, 4, 4, 2, 0, 2, 4, 4, 4, 0, 2, 2, 2, 4, 2, 2, 1, 1, 0, 4, 0, 4, 0, 4, 0, 2, 0, 4, 1, 0, 4, 0, 0, 4, 0, 0, 4, 0, 1, 0, 0, 1, 4, 0, 2, 1, 4, 4, 1, 4, 4, 0, 0, 4, 0, 4, 2, 1, 0, 4, 2, 1, 2, 2, 0, 0, 2, 0, 4, 2, 4, 0, 2, 2, 0, 2, 0])  Evaluating Silhouette Score score = silhouette_score(df.drop(columns=drop_cols), pred, metric=\u0026#39;euclidean\u0026#39;) score 0.43527407005177665  # Adding predicted cluster values to df df = df.copy() df[\u0026#39;Cluster\u0026#39;] = pred Visualizing Clusters sns.scatterplot(data=df, x=\u0026#39;GCA/SCA\u0026#39;, y=\u0026#39;SCA\u0026#39;, hue=\u0026#39;Cluster\u0026#39;, palette = \u0026#39;colorblind\u0026#39;); \r\r\rsns.scatterplot(data=df, x=\u0026#39;GCA/SCA PassLive\u0026#39;, y=\u0026#39;SCA\u0026#39;, hue=\u0026#39;Cluster\u0026#39;, palette = \u0026#39;colorblind\u0026#39;); \r\r\rsns.scatterplot(data=df, x=\u0026#39;GCA/SCA Drib\u0026#39;, y=\u0026#39;GCA/SCA Sh\u0026#39;, hue=\u0026#39;Cluster\u0026#39;, palette = \u0026#39;colorblind\u0026#39;); \r\r\rsns.scatterplot(data=df, x=\u0026#39;GCA/SCA Drib\u0026#39;, y=\u0026#39;GCA/SCA Def\u0026#39;, hue=\u0026#39;Cluster\u0026#39;, palette = \u0026#39;colorblind\u0026#39;); \r\r\rViewing Clusters # Cluster 0 df.loc[df[\u0026#39;Cluster\u0026#39;]==0]  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Rk Player Nation Pos Squad Age Born 90s SCA SCA90 ... GCA Def Ratio GCA OG Ratio GCA/SCA PassLive GCA/SCA PassDead GCA/SCA Drib GCA/SCA Sh GCA/SCA Fld GCA/SCA Def GCA/SCA Cluster     5 6 Sergio Agüero ARG FW Manchester City 32 1988 2.3 5 2.22 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 0   18 19 Dele Alli ENG MF Tottenham 24 1996 1.6 7 4.41 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 0   25 26 Elliot Anderson SCO MF Newcastle Utd 18 2002 0.0 0 0.00 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 0   26 27 Felipe Anderson BRA FWMF West Ham 27 1993 0.0 0 0.00 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 0   33 34 Charlie Austin ENG FW West Brom 31 1989 1.5 1 0.65 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 0   ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ...   490 491 Joe Willock ENG MF Newcastle Utd 21 1999 3.9 10 2.59 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 0   492 493 Harry Winks ENG MF Tottenham 25 1996 6.3 6 0.95 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 0   497 498 Okay Yokuşlu TUR MF West Brom 26 1994 2.9 3 1.04 ... 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.333333 0   500 501 Andi Zeqiri SUI FWDF Brighton 21 1999 0.8 2 2.65 ... 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.500000 0   504 505 Martin Ødegaard NOR MF Arsenal 22 1998 2.6 5 1.94 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 0    103 rows × 46 columns\n # Cluster 1 df.loc[df[\u0026#39;Cluster\u0026#39;]==1]  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Rk Player Nation Pos Squad Age Born 90s SCA SCA90 ... GCA Def Ratio GCA OG Ratio GCA/SCA PassLive GCA/SCA PassDead GCA/SCA Drib GCA/SCA Sh GCA/SCA Fld GCA/SCA Def GCA/SCA Cluster     46 47 Harvey Barnes ENG FWMF Leicester City 23 1997 21.6 68 3.15 ... 0.000000 0.0 0.113636 0.000000 0.250000 0.000000 0.125000 0.000000 0.117647 1   118 119 Raphael Dias Belloli BRA MF Leeds United 24 1996 18.0 80 4.45 ... 0.000000 0.0 0.108696 0.105263 0.000000 0.500000 0.000000 0.000000 0.112500 1   145 146 Roberto Firmino BRA FW Liverpool 29 1991 22.8 75 3.28 ... 0.000000 0.0 0.109091 0.000000 0.000000 0.125000 0.000000 0.000000 0.093333 1   174 175 Pascal Groß GER MF Brighton 29 1991 15.9 68 4.29 ... 0.000000 0.0 0.095238 0.041667 0.000000 0.000000 0.000000 0.000000 0.073529 1   177 178 İlkay Gündoğan GER MF Manchester City 30 1990 17.2 61 3.54 ... 0.125000 0.0 0.068182 0.000000 0.500000 0.000000 0.500000 0.500000 0.131148 1   181 182 Jack Harrison ENG MF Leeds United 24 1996 21.7 70 3.23 ... 0.100000 0.0 0.118644 0.000000 1.000000 0.000000 0.000000 0.500000 0.142857 1   191 192 Son Heung-min KOR FW Tottenham 28 1992 23.9 75 3.14 ... 0.000000 0.0 0.173913 0.105263 0.166667 0.000000 0.000000 0.000000 0.146667 1   196 197 Callum Hudson-Odoi ENG FWDF Chelsea 20 2000 9.1 44 4.81 ... 0.000000 0.0 0.119048 0.000000 0.500000 0.000000 0.000000 0.000000 0.136364 1   221 222 Harry Kane ENG FW Tottenham 27 1993 22.4 75 3.35 ... 0.000000 0.0 0.234043 0.500000 0.307692 0.000000 0.250000 0.000000 0.240000 1   263 264 Ademola Lookman ENG FWMF Fulham 23 1997 21.4 90 4.21 ... 0.000000 0.0 0.096774 0.125000 1.000000 0.000000 0.000000 0.000000 0.111111 1   270 271 James Maddison ENG MFFW Leicester City 24 1996 18.1 82 4.53 ... 0.090909 0.0 0.162791 0.050000 0.000000 0.000000 0.285714 0.250000 0.134146 1   272 273 Riyad Mahrez ALG FW Manchester City 30 1991 15.4 56 3.63 ... 0.000000 0.0 0.121951 0.166667 0.333333 0.000000 0.000000 0.000000 0.125000 1   276 277 Sadio Mané SEN FW Liverpool 28 1992 21.8 85 3.90 ... 0.125000 0.0 0.049180 0.000000 0.000000 0.428571 0.083333 1.000000 0.094118 1   294 295 John McGinn SCO MF Aston Villa 26 1994 23.0 57 2.48 ... 0.000000 0.0 0.130435 1.000000 0.500000 0.500000 0.166667 0.000000 0.175439 1   314 315 Mason Mount ENG MFFW Chelsea 22 1999 22.7 109 4.79 ... 0.000000 0.0 0.035714 0.108108 0.142857 0.500000 0.250000 0.000000 0.091743 1   325 326 Pedro Neto POR FWMF Wolves 20 2000 24.0 98 4.09 ... 0.000000 0.0 0.058824 0.090909 0.083333 0.500000 0.000000 0.000000 0.081633 1   367 368 Marcus Rashford ENG FW Manchester Utd 23 1997 23.6 72 3.05 ... 0.000000 0.0 0.192308 0.000000 0.083333 0.000000 0.166667 0.000000 0.166667 1   396 397 Bukayo Saka ENG FWDF Arsenal 19 2001 20.7 72 3.48 ... 0.000000 0.0 0.119048 0.000000 0.125000 0.000000 0.250000 0.000000 0.111111 1   398 399 Mohamed Salah EGY FW Liverpool 28 1992 23.4 73 3.13 ... 0.000000 0.0 0.096154 0.000000 0.000000 0.222222 0.333333 0.000000 0.123288 1   416 417 Bernardo Silva POR MFFW Manchester City 26 1994 16.5 54 3.28 ... 0.000000 0.0 0.121951 0.000000 0.000000 0.333333 0.000000 0.000000 0.111111 1   431 432 Raheem Sterling ENG FW Manchester City 26 1994 20.9 73 3.50 ... 0.000000 0.0 0.125000 0.000000 0.181818 0.333333 0.125000 0.000000 0.150685 1   446 447 Youri Tielemans BEL MF Leicester City 23 1997 25.3 58 2.29 ... 0.090909 0.0 0.187500 0.000000 0.000000 0.000000 0.500000 0.333333 0.189655 1   454 455 Adama Traoré ESP FWDF Wolves 25 1996 18.8 79 4.20 ... 0.000000 0.0 0.000000 0.000000 0.000000 0.333333 0.111111 0.000000 0.025316 1   457 458 Leandro Trossard BEL FWMF Brighton 26 1994 19.3 72 3.73 ... 0.000000 0.0 0.100000 0.111111 0.000000 0.166667 0.000000 0.000000 0.097222 1   477 478 Ollie Watkins ENG FW Aston Villa 25 1995 24.0 67 2.79 ... 0.000000 0.0 0.140000 0.000000 0.000000 0.375000 0.000000 0.000000 0.149254 1   482 483 Ashley Westwood ENG MF Burnley 30 1990 25.9 61 2.36 ... 0.000000 0.0 0.042553 0.076923 0.000000 0.000000 0.000000 0.000000 0.049180 1    26 rows × 46 columns\n # Cluster 2 df.loc[df[\u0026#39;Cluster\u0026#39;]==2]  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Rk Player Nation Pos Squad Age Born 90s SCA SCA90 ... GCA Def Ratio GCA OG Ratio GCA/SCA PassLive GCA/SCA PassDead GCA/SCA Drib GCA/SCA Sh GCA/SCA Fld GCA/SCA Def GCA/SCA Cluster     2 3 Che Adams ENG FW Southampton 24 1996 21.2 42 1.98 ... 0.000000 0.0 0.166667 0.000000 0.000000 0.0 0.142857 0.000000 0.142857 2   10 11 Marc Albrighton ENG FWDF Leicester City 31 1989 12.9 42 3.26 ... 0.000000 0.0 0.156250 0.000000 0.000000 1.0 0.000000 0.000000 0.142857 2   11 12 Thiago Alcántara ESP MF Liverpool 29 1991 10.9 31 2.84 ... 0.000000 0.0 0.000000 0.000000 0.000000 0.0 0.000000 0.000000 0.000000 2   19 20 Miguel Almirón PAR MFFW Newcastle Utd 27 1994 17.6 46 2.62 ... 0.000000 0.0 0.083333 0.000000 0.000000 0.0 0.000000 0.000000 0.065217 2   29 30 Stuart Armstrong SCO MF Southampton 28 1992 19.2 46 2.39 ... 0.111111 0.0 0.156250 0.000000 0.400000 0.0 0.333333 0.333333 0.195652 2   ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ...   491 492 Callum Wilson ENG FW Newcastle Utd 29 1992 20.1 47 2.33 ... 0.000000 0.0 0.222222 0.000000 0.000000 1.0 0.230769 0.000000 0.212766 2   494 495 Granit Xhaka SUI MF Arsenal 28 1992 20.1 28 1.40 ... 0.000000 0.0 0.076923 0.000000 0.000000 0.0 0.000000 0.000000 0.071429 2   498 499 Wilfried Zaha CIV FW Crystal Palace 28 1992 18.5 54 2.92 ... 0.000000 0.0 0.205882 0.000000 0.200000 0.0 0.000000 0.000000 0.166667 2   499 500 Andre-Frank Zambo Anguissa CMR MF Fulham 25 1995 20.9 42 2.01 ... 0.000000 0.0 0.093750 0.000000 0.285714 0.0 0.000000 0.000000 0.119048 2   502 503 Hakim Ziyech MAR FWMF Chelsea 27 1993 8.0 46 5.73 ... 0.000000 0.0 0.142857 0.166667 0.000000 0.0 0.000000 0.000000 0.130435 2    66 rows × 46 columns\n # Cluster 3 # Top performing EPL players! df.loc[df[\u0026#39;Cluster\u0026#39;]==3]  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Rk Player Nation Pos Squad Age Born 90s SCA SCA90 ... GCA Def Ratio GCA OG Ratio GCA/SCA PassLive GCA/SCA PassDead GCA/SCA Drib GCA/SCA Sh GCA/SCA Fld GCA/SCA Def GCA/SCA Cluster     111 112 Kevin De Bruyne BEL MF Manchester City 29 1991 17.4 108 6.20 ... 0.0625 0.0 0.185714 0.000000 0.166667 0.000000 0.250000 0.166667 0.148148 3   141 142 Bruno Fernandes POR MF Manchester Utd 26 1994 24.4 121 4.97 ... 0.0000 0.0 0.209302 0.125000 0.000000 0.285714 0.000000 0.000000 0.181818 3   172 173 Jack Grealish ENG FWMF Aston Villa 25 1995 22.0 134 6.10 ... 0.0500 0.0 0.156863 0.000000 0.000000 0.000000 0.333333 1.000000 0.149254 3   232 233 Mateusz Klich POL MF Leeds United 30 1990 22.5 86 3.82 ... 0.0000 0.0 0.140845 0.142857 0.500000 0.000000 0.000000 0.000000 0.139535 3    4 rows × 46 columns\n # Cluster 4 df.loc[df[\u0026#39;Cluster\u0026#39;]==4]  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Rk Player Nation Pos Squad Age Born 90s SCA SCA90 ... GCA Def Ratio GCA OG Ratio GCA/SCA PassLive GCA/SCA PassDead GCA/SCA Drib GCA/SCA Sh GCA/SCA Fld GCA/SCA Def GCA/SCA Cluster     1 2 Tammy Abraham ENG FW Chelsea 23 1997 11.3 18 1.59 ... 0.0 0.0 0.090909 0.0 0.0 0.000000 0.333333 0.0 0.111111 4   17 18 Allan BRA MF Everton 30 1991 12.4 17 1.37 ... 0.0 0.0 0.083333 0.0 0.0 0.000000 0.000000 0.0 0.058824 4   21 22 Steven Alzate COL MFDF Brighton 22 1998 8.0 21 2.63 ... 0.0 0.0 0.058824 0.0 0.0 0.000000 0.000000 0.0 0.047619 4   27 28 Michail Antonio ENG FW West Ham 30 1990 13.8 40 2.90 ... 0.0 0.0 0.210526 1.0 0.0 0.166667 0.181818 0.0 0.200000 4   34 35 Jordan Ayew GHA FWMF Crystal Palace 29 1991 15.6 26 1.67 ... 0.0 0.0 0.294118 0.0 0.0 1.000000 0.200000 0.0 0.269231 4   ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ...   469 470 Matěj Vydra CZE FW Burnley 28 1992 7.0 18 2.58 ... 0.0 0.0 0.111111 0.0 0.0 0.000000 0.000000 0.0 0.055556 4   471 472 Theo Walcott ENG MFFW Southampton 31 1989 13.7 20 1.46 ... 0.0 0.0 0.066667 0.0 0.0 0.500000 0.500000 0.0 0.150000 4   480 481 Danny Welbeck ENG FW Brighton 30 1990 8.2 22 2.69 ... 0.0 0.0 0.000000 0.0 0.0 0.000000 0.500000 0.0 0.045455 4   493 494 Chris Wood NZL FW Burnley 29 1991 18.9 24 1.27 ... 0.0 0.0 0.047619 0.0 0.0 1.000000 0.000000 0.0 0.083333 4   495 496 Andriy Yarmolenko UKR FWMF West Ham 31 1989 4.0 14 3.53 ... 0.0 0.0 0.181818 0.0 0.0 0.000000 0.000000 0.0 0.142857 4    80 rows × 46 columns\n Interpretation The information above can be used for recruiting or creating tactics.\nIf I was recruiting players to make a great counterattacking team, I would look at GCA/SCA Def (ratio of defensive actions that led to a goal over defensive actions that led to a shot attempt) and GCA/SCA Drib (ratio of successful dribbles that led to a goal over successful dribbles that led to a shot attempt). A high value in GCA/SCA Def or GCA/SCA Drib would mean that the player\u0026rsquo;s defensive action or dribbles often lead to goals.\nIf I was recruiting players to make a great possession-based team, I would look at GCA/SCA LivePass (ratio of completed live-ball passes that led to a goal over completed live-ball passes that led to a shot) and GCA/SCA Sh (ratio of shots that led to another goal-scoring shot over shots that led to another shot attempt).\nThen, I can sort my data values by the interested column in descending order and analyze clusters that showed at the top. The players in those clusters would be my initial candidates for transfer targets. The other players from the same cluster might not have the highest values in the interested column, but the model grouped those players because they were similar in other performance metrics to the players that had the highest values in the interested column.\nSimilarly, if I have a player who is injured, and I need to recruit a player who can fill his role, I can look at other players from the injured player\u0026rsquo;s cluster and try to recruit the player who is most cost-effective and most similar to the injured player.\nAnother way to utilize the model\u0026rsquo;s information would be to look at visualizations. For example, SCA and GCA ratios help identify players' playstyle. If a player has a high SCA LivePass Ratio, that would mean that most of the player\u0026rsquo;s Shot Creating Actions are from completed live-ball passes. If I want to implement a possession-based tactic because the opponent team is weak to possession-based playstyle, it makes sense to start players who have both high GCA/SCA LivePass and GCA/SCA Sh ratios. However, my team may not have a lot of players with both GCA/SCA LivePass and GCA/SCA Sh ratios. What I can do is make a scatter plot of GCA/SCA LivePass vs. GCA/SCA Sh ratios and visually confirm which clusters or which specific players showed up in the middle of the plot. Then I can inspect those clusters to find players most similar to the player that had both high GCA/SCA LivePass and SCA/SCA Sh ratios.\nUsing the information from the clustering model, the coach can start or recruit certain players to exploit opponent team\u0026rsquo;s weaknesses.\n","permalink":"https://junschoi.github.io/posts/clustering_epl_players/","tags":["kmeans","epl","sklearn","clustering"],"title":"Clustering English Premier League Forwards and Midfielders using K-Means"},{"categories":["Python"],"contents":"When I used to work as a freight forwarder, I had to deal with a lot of scanned documents. Because scanned documents were often in image format, I wasn\u0026rsquo;t able to just copy paste the contents for my tasks so my work involving scanned documents took way longer than documents that were in normal text format. If you are having a similar problem and don\u0026rsquo;t want to type out everything from the document, perhaps this post will help you.\nAbout OCR and Tesseract Optical character recognition or optical character reader (OCR) is the electronic or mechanical conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (for example the text on signs and billboards in a landscape photo) or from subtitle text superimposed on an image (for example: from a television broadcast).[1]\nMany people think OCR problem is solved, but it is actually a very challenging problem because images containing the text could have a lot of distortions, noise, different languages and fonts.\nOne of the most popular OCR engines is Tesseract. Tesseract began as a Ph.D. research project in HP Labs, Bristol. It gained popularity and was developed by HP between 1984 and 1994. In 2005 HP released Tesseract as an open-source software. Since 2006 it is developed by Google.[2]\nIn this demo, I will show you how to install Tesseract on Windows and perform simple OCR using pytesseract (Python wrapper for Tesseract) and Pillow (Open-source and free Python imaging library).\nNote\nI am assuming you are using Windows 10 and have Python 3 installed on your computer.\n Installing Tesseract (Windows 10) Download Tesseract You can download Tesseract installer for Windows here.\nAfter you download the installer, run the executable file. You can also download other language packs during the installation process.\n\r\r\rAdd Tesseract to Path Variable   Search for \u0026ldquo;Environment Variable\u0026rdquo; on Windows search bar\n  Click on \u0026ldquo;Environment Variables\u0026rdquo; button\n  Create new path under user variables\n  Enter the path where you installed Tesseract\n  \r\r\rInstalling pytesseract and pillow Run the following code in command line to install pytesseract and pillow which are Python libraries needed for this demo.\npip install pytesseract pip install pillow Now you are ready to OCR!\nSimple OCR Demo Testing image 1:\n\r\r\rImage source\n# Importing libraries from PIL import Image import pytesseract print(pytesseract.image_to_string(Image.open(\u0026#39;tess_test.png\u0026#39;), lang=\u0026#39;eng\u0026#39;)) A Python Approach to Character Recognition  Testing image 2:\n\r\r\rImage source\nprint(pytesseract.image_to_string(Image.open(\u0026#39;tess_test2.jpg\u0026#39;), lang=\u0026#39;kor\u0026#39;)) 안녕하세요  Testing image 3:\n\r\r\rImage source\nprint(pytesseract.image_to_string(Image.open(\u0026#39;tess_test3.jpg\u0026#39;), lang=\u0026#39;jpn\u0026#39;)) こんにちは  Tesseract works fine for clean text images, but it will often not work properly when the images are noisy, have obscure font, complex background, etc.\nTesting image 4:\n\r\r\rImage source\nprint(pytesseract.image_to_string(Image.open(\u0026#39;tess_test4.jpg\u0026#39;), lang=\u0026#39;eng\u0026#39;)) WHitg Um Alive Tl MAKE TINY CHANGES To EARTH  Additional Resources There is a more detailed guide on how to use pytesseract on this website. It also shows how to preprocess images with cv2 library to increase OCR performance.\nReferences [1]: https://en.wikipedia.org/wiki/Optical_character_recognition\n[2]: https://nanonets.com/blog/ocr-with-tesseract/\n","permalink":"https://junschoi.github.io/posts/tesseract_demo/","tags":["ocr","tesseract","pillow"],"title":"Optical Character Recognition using Python and Tesseract"},{"categories":["Technology"],"contents":"For people who want to browse the web without using the mouse, I recommend getting Vimium extension for Google Chrome or Firefox. I\u0026rsquo;ve been using Vimium extension since May 4th, 2020, and it drastically improved my web browsing experience. If you are also thinking about changing from full-sized or 80% (ten keyless) keyboard to 60% or 40% keyboard, I recommend getting Vimium or a similiar type of extention since you\u0026rsquo;ll be operating with less keys.\nVimium provides keyboard shortcuts for navigation and control in the spirit of Vim. You can install Vimium for Google Chrome here.\nThere are other extensions similar to Vimium like cVim and Surfingkeys. However, after trying out all of them, I found Vimium to be the best option for the reason below.\ncVim\u0026rsquo;s f key, which is my most used key that opens a link in the current tab, did not work properly on Google Chrome. Also Surfingkeys' f key sometimes did not detect certain elements on some websites. For this reason alone, I decided to go with Vimium.\nMost of information below can be found here, but I\u0026rsquo;ve added some info that were not in the official wiki page and will be going over some features more thoroughly.\nBasic keyboard shortcuts on Vimium Navigating the current page:\n? show the help dialog for a list of all available keys h scroll left j scroll down k scroll up l scroll right gg scroll to top of the page G scroll to bottom of the page d scroll down half a page u scroll up half a page f open a link in the current tab F open a link in a new tab r reload gs view source i enter insert mode -- all commands will be ignored until you hit Esc to exit yy copy the current url to the clipboard yf copy a link url to the clipboard gf cycle forward to the next frame gF focus the main/top frame Navigating to new pages:\no Open URL, bookmark, or history entry O Open URL, bookmark, history entry in a new tab b Open bookmark B Open bookmark in a new tab Using find:\n/ enter find mode -- type your search query and hit enter to search, or Esc to cancel n cycle forward to the next find match N cycle backward to the previous find match Navigating history:\nH go back in history L go forward in history Manipulating tabs:\nJ, gT go one tab left K, gt go one tab right g0 go to the first tab g$ go to the last tab ^ visit the previously-visited tab t create tab yt duplicate current tab x close current tab X restore closed tab (i.e. unwind the \u0026#39;x\u0026#39; command) T search through your open tabs W move current tab to new window \u0026lt;a-p\u0026gt; pin/unpin current tab Using marks:\nma, mA set local mark \u0026#34;a\u0026#34; (global mark \u0026#34;A\u0026#34;) `a, `A jump to local mark \u0026#34;a\u0026#34; (global mark \u0026#34;A\u0026#34;) `` jump back to the position before the previous jump -- that is, before the previous gg, G, n, N, / or `a Additional advanced browsing commands:\n]], [[ follow the link labeled \u0026#39;next\u0026#39; or \u0026#39;\u0026gt;\u0026#39; (\u0026#39;previous\u0026#39; or \u0026#39;\u0026lt;\u0026#39;) \u0026lt;a-f\u0026gt; open multiple links in a new tab gi focus the first (or n-th) text input box on the page gu go up one level in the URL hierarchy gU go up to root of the URL hierarchy ge edit the current URL gE edit the current URL and open in a new tab zH scroll all the way left zL scroll all the way right v enter visual mode; use p/P to paste-and-go, use y to yank V enter visual line mode Notes:\nMany Vimium commands accept count prefix.\nFor example,\n All scrolling commands like j \u0026amp; k. 2j and holding j would scroll down twice as fast while smooth scrolling is enabled. Commands that open new tabs or windows like t \u0026amp; W. 2yt would open two new current tabs to the right of current tab. H \u0026amp; L.  Custom key mappings If you wish to assign custom key mappings, you can do so on Vimium\u0026rsquo;s options under \u0026ldquo;Custom key mappings\u0026rdquo;.\nCustom key commands:\nmap key command maps a key to a Vimium command. unmap key unmaps a key. unmapAll unmaps all bindings. Examples (advanced custom commands included):\nmap r reload maps the r key to reloading the page unmap r removes any mapping for the r key map C LinkHints.activateMode action=hover maps C key to mouse hovering action map S LinkHints.activateMode action=focus maps S key to element focus action map # LinkHints.activateMode action=copy-text maps # key to copy-text action map \u0026lt;a-,\u0026gt; moveTabLeft count=99 maps ALT-, to move current tab to leftmost map \u0026lt;a-.\u0026gt; moveTabRight count=99 maps ALT-. to move current tab to rightmost map X toggleMuteTab All maps X to toggle mute on all tabs map Y toggleMuteTab other maps Y to toggle mute except current tab map X createTab http://www.bbc.com/news maps X to open specified URL in new tab map X createTab window [url_1][url_2] maps X to open url_1 \u0026amp; url_2 in new window map X createTab window maps X to open new window map X createTab incognito maps X to open incongito window Special keys:\n\u0026lt;c-*\u0026gt;, \u0026lt;a-*\u0026gt;, \u0026lt;m-*\u0026gt; for control, alt, and meta (command on Mac) \u0026lt;left\u0026gt;, \u0026lt;right\u0026gt;, \u0026lt;up\u0026gt;, \u0026lt;down\u0026gt; for the arrow keys \u0026lt;f1\u0026gt; through \u0026lt;f12\u0026gt; for the function keys \u0026lt;space\u0026gt; for the space key \u0026lt;tab\u0026gt; for the tab key \u0026lt;enter\u0026gt; for the enter key \u0026lt;delete\u0026gt; for the delete key \u0026lt;backspace\u0026gt; for the backspace key \u0026lt;insert\u0026gt; for the insert key \u0026lt;home\u0026gt; for the home key \u0026lt;end\u0026gt; for the end key Disabling Vimium Partially disabling Vimium In this case, Vimium is enabled as normal, except for the three listed keys. These are passed through to the underlying page. Here, we\u0026rsquo;re using Gmail\u0026rsquo;s native j/k bindings, and Gmail\u0026rsquo;s native help screen. All other Vimium keys are handled by Vimium, as usual.\nCompletely disabling Vimium In this case, Vimium is completely disabled since the list of excluded keys is empty.\nTemporarily disabling Vimium In insert mode, all Vimium bindings are temporarily disabled, and all keystrokes (except Escape) are passed through to the underlying page. Enter insert mode with i, and leave insert mode with Escape.\nPersonally, I enable Vimium on every website and enter insert mode to use the webpage\u0026rsquo;s native or another extension\u0026rsquo;s shortcut keys (for example, shortcuts for Reddit Enhancement Suite, Gmail, Jupyter Notebook, Youtube, etc). It\u0026rsquo;s worth noting that when you have both Reddit Enhancement Suite extension and Vimium extension installed, Vimium takes over Reddit Enhancement Suite shortcuts by default, and you have to use insert mode to use Reddit Enhancement Suite\u0026rsquo;s shortcuts.\npassNextKey Vimium uses the Escape key to exit insert mode. Some web sites use the Escape key to perform actions (like closing a window) from within inputs. For such web sites, Vimium effectively blocks the site’s own functionality. One example of this is the Messenger popups used by Facebook.\nThe passNextKey command solves this problem.\nmap \u0026lt;c-]\u0026gt; passNextKey Now, CTRL-]-Escape passes the Escape key to the web page itself. In the case of Facebook, this closes the Messenger popup.\nAnother example would be if you want to use Youtube\u0026rsquo;s fullscreen shortcut f without going into insert mode since entering insert mode creates a message box on the bottom right corner of the screen. You would enter CTRL-]-F to fullscreen Youtube video without going into insert mode.\nVisual mode \u0026amp; carat mode Visual mode is used for selecting text on the page and caret mode is used for changing the starting point for selecting text on the page.\nPersonally, I do not find carat mode c to be all that useful since I can just use Vimium\u0026rsquo;s find function / or Chrome\u0026rsquo;s native find function CTRL-F to select the starting text position.\nHowever, I do find Vimium\u0026rsquo;s visual mode v and visual line mode V to be quite useful since I can implement number of vim-like movements after selecting starting text position. For example: j, k, h, l, w, e, b, etc. Counts are also supported; for example, 3e.\nExiting visual mode:\nesc exit visual mode y yank the selected text to the clipboard p/P send the selected text to the default search engine in current tab/new tab In visual mode, o swaps the anchor and the focus, so you can also move the \u0026ldquo;other end\u0026rdquo; of the selection.\nScrolling in visual mode:\n\u0026lt;c-y\u0026gt; scroll up \u0026lt;c-e\u0026gt; scroll down Personal comments about Vimium I\u0026rsquo;ve been using Vimium and Chrome shortcuts together. If there\u0026rsquo;s a shortcut that is supported by Google Chrome, I tend to use Chrome\u0026rsquo;s native shortcut version over Vimium\u0026rsquo;s.\nFor example,\n I still use native Chrome shortcuts to switch tabs (CTRL-NUM \u0026amp; CTRL-PAGEUP/PAGEDOWN) but I also started to use Vimium\u0026rsquo;s J and K to switch tabs. I use Chrome\u0026rsquo;s native shortcut CTRL-SHIFT-N to open a new incognito tab, and I have not made a custom key for opening a new incognito tab in Vimium.  Overall, I recommend Vimium to anyone who wants to improve web-browsing experience and doesn\u0026rsquo;t mind putting some time into learning new shortcuts.\n","permalink":"https://junschoi.github.io/posts/vimium_chrome/","tags":["vimium","chrome"],"title":"Vimium for Google Chrome"},{"categories":["Python"],"contents":"\r\r\rHave you ever had this problem where foreign languages appear in gibberish?\nMojibake (文字化け) occurs when someone has encoded Unicode with one standard and decoded it with a different one. The result is the garbled text that you see in the picture above. In Japanese, the word literally translates to “character changing”.\nThis was a problem back when Unicode wasn’t around and different countries were using their own encoding systems. The problem got worse when East Asian countries started to develop multi-byte encoding systems to include hundreds and thousands of their characters.\nThankfully, there is a python package that can easily fix Mojibake!\nftfy: fixes text for you Github: https://github.com/LuminosoInsight/python-ftfy\nDocumentation: https://ftfy.readthedocs.io/en/latest/\nAccording to the documentation, ftfy can understand text that was decoded as any of the following encodings:\n Latin-1 (ISO-8859–1) Windows-1252 (cp1252 — used in Microsoft products) Windows-1251 (cp1251 — the Russian version of cp1252) Windows-1250 (cp1250 — the Eastern European version of cp1252) ISO-8859–2 (which is not quite the same as Windows-1250) MacRoman (used on Mac OS 9 and earlier) cp437 (used in MS-DOS and some versions of the Windows command prompt)  When it was actually intended to be decoded as one of these variable-length encodings:\n UTF-8 CESU-8 (what some people think is UTF-8)  Installing ftfy To install ftfy, run the following pip command:\npip install ftfy Using ftfy The main method of ftfy is the fix_text method.\nDocumentation description: Given Unicode text as input, fix inconsistencies and glitches in it, such as mojibake.\nTo use this method, simply import ftfy and call the function on the characters that you wish to ungarble!\nimport ftfy ftfy.fix_text(\u0026#39;This text should be in â€œquotesâ€\\x9d.\u0026#39;) 'This text should be in \u0026quot;quotes\u0026quot;.'  There is another method called explain_unicode that can be used to break down character by character to see its category in the Unicode standard and its name in the Unicode standard.\nftfy.explain_unicode(\u0026#39;(╯°□°)╯︵ ┻━┻\u0026#39;) U+0028 ( [Ps] LEFT PARENTHESIS U+256F ╯ [So] BOX DRAWINGS LIGHT ARC UP AND LEFT U+00B0 ° [So] DEGREE SIGN U+25A1 □ [So] WHITE SQUARE U+00B0 ° [So] DEGREE SIGN U+0029 ) [Pe] RIGHT PARENTHESIS U+256F ╯ [So] BOX DRAWINGS LIGHT ARC UP AND LEFT U+FE35 ︵ [Ps] PRESENTATION FORM FOR VERTICAL LEFT PARENTHESIS U+0020 [Zs] SPACE U+253B ┻ [So] BOX DRAWINGS HEAVY UP AND HORIZONTAL U+2501 ━ [So] BOX DRAWINGS HEAVY HORIZONTAL U+253B ┻ [So] BOX DRAWINGS HEAVY UP AND HORIZONTAL  Enjoy fixing Mojibakes!\n","permalink":"https://junschoi.github.io/posts/ftfy_guide/","tags":["ftfy","mojibake"],"title":"Fixing Mojibake using Python and ftfy"},{"categories":["Python"],"contents":"If you use iTunes to listen to your music, you can export your iTunes library data as an XML file!\n Open iTunes File \u0026gt; Export Library\u0026hellip; Save the file at your desired directory  \r\r\rThere are many ways to parse an XML file, but in this demo, I will be using xml.etree.ElementTree module to parse it. You can read more about the module here: https://docs.python.org/3/library/xml.etree.elementtree.html\nIf you open the XML file with a text editor, you will see that the data inside the XML file is structured like the screenshot below.\n\r\r\rParsing iTunes XML file with Python # Importing libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.font_manager as fm import matplotlib from matplotlib.ticker import FormatStrFormatter from xml.etree import ElementTree as ET from collections import defaultdict # Accessing data content tree = ET.parse(\u0026#39;Library.xml\u0026#39;) root = tree.getroot() root[0] will allow us to access contents of root\n# Exploring contents of root root_lst = [] for i, x in enumerate(root[0]): root_lst.append([x.text, x.tag]) pd.DataFrame(root_lst, columns=[\u0026#39;text\u0026#39;, \u0026#39;tag\u0026#39;])  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  text tag     0 Major Version key   1 1 integer   2 Minor Version key   3 1 integer   4 Date key   5 2020-12-27T21:49:35Z date   6 Application Version key   7 12.11.0.26 string   8 Features key   9 5 integer   10 Show Content Ratings key   11 None true   12 Music Folder key   13 file://localhost/C:/Users/junsc/Music/iTunes/i... string   14 Library Persistent ID key   15 64B0A8775DD8CA31 string   16 Tracks key   17 \\n\\t\\t dict   18 Playlists key   19 \\n\\t\\t array     Next, I will be accessing contents of \u0026ldquo;Tracks\u0026rdquo; node, which contains all the tracks and its data. You will see that each track data is stored in every odd numbered index of \u0026ldquo;Tracks\u0026rdquo; node.\n# Exploring \u0026#34;Tracks\u0026#34; node track_lst = [] for x in root[0][17]: track_lst.append([x.text, x.tag]) pd.DataFrame(track_lst, columns = [\u0026#39;text\u0026#39;, \u0026#39;tag\u0026#39;])  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  text tag     0 2571 key   1 \\n\\t\\t\\t dict   2 2573 key   3 \\n\\t\\t\\t dict   4 2575 key   ... ... ...   13443 \\n\\t\\t\\t dict   13444 16015 key   13445 \\n\\t\\t\\t dict   13446 16017 key   13447 \\n\\t\\t\\t dict    13448 rows × 2 columns\n Next, we will be parsing the contents inside the first track dict.\n\r\r\rSimilarly to the \u0026ldquo;Tracks\u0026rdquo; node, you will see that the name of each track feature is stored in even indices while the actual value of each track feature is stored in odd indices.\n# Exploring contents of the first track dict song_lst = [] for x in root[0][17][1]: # [1] because we are parsing only the first track song_lst.append([x.text, x.tag]) pd.DataFrame(song_lst, columns=[\u0026#39;text\u0026#39;, \u0026#39;tag\u0026#39;])  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  text tag     0 Track ID key   1 2571 integer   2 Name key   3 Colors string   4 Artist key   ... ... ...   61 726B801143FC336C string   62 Track Type key   63 Remote string   64 Apple Music key   65 None true    66 rows × 2 columns\n If you examine the XML file more carefully, you will notice that not all tracks contain the same number of features. For example, some classical songs might include \u0026ldquo;Movement Count\u0026rdquo;, \u0026ldquo;Movement Name\u0026rdquo;, and \u0026ldquo;Movement Number\u0026rdquo; features, but those features might not be in tracks from other genres.\nTo include all features from every single song in the library, I will be parsing through every single track and its features in the \u0026ldquo;Tracks\u0026rdquo; node using list comprehension.\ncolumns = [ root[0][17][i][j].text for i, _ in enumerate(root[0][17]) for j, _ in enumerate(root[0][17][i]) if root[0][17][i][j].tag == \u0026#39;key\u0026#39; ] columns = sorted(list(set(columns))) # List of all columns in the library columns ['Album', 'Album Artist', 'Album Loved', 'Apple Music', 'Artist', 'Artwork Count', 'Bit Rate', 'Clean', 'Comments', 'Compilation', 'Composer', 'Date Added', 'Date Modified', 'Disc Count', 'Disc Number', 'Explicit', 'Genre', 'Grouping', 'HD', 'Has Video', 'Kind', 'Loved', 'Matched', 'Movement Count', 'Movement Name', 'Movement Number', 'Music Video', 'Name', 'Part Of Gapless Album', 'Persistent ID', 'Play Count', 'Play Date', 'Play Date UTC', 'Playlist Only', 'Release Date', 'Sample Rate', 'Size', 'Skip Count', 'Skip Date', 'Sort Album', 'Sort Album Artist', 'Sort Artist', 'Sort Composer', 'Sort Name', 'Total Time', 'Track Count', 'Track ID', 'Track Number', 'Track Type', 'Work', 'Year']  Also, you will notice that some features of the track have boolean values in the XML files. If you try to access the boolean value by using .text attribute, you will get \u0026ldquo;None\u0026rdquo; the boolean values are in .tag attribute.\nI will be using collections.defaultdict to store all the data in \u0026ldquo;Tracks\u0026rdquo; node of the XML file. Each feature will the key of defaultdict and the value will be a list containing all the track values for that feature. If the track we\u0026rsquo;re iterating doesn\u0026rsquo;t have a particular feature, np.nan will be added to the feature list.\nIf you don\u0026rsquo;t add np.nan, you will get an \u0026ldquo;ValueError: arrays must all be same length\u0026rdquo; when you try to create pandas DataFrame object later.\ndata = defaultdict(list) bool_columns = [ \u0026#39;Album Loved\u0026#39;, \u0026#39;Apple Music\u0026#39;, \u0026#39;Clean\u0026#39;, \u0026#39;Compilation\u0026#39;, \u0026#39;Explicit\u0026#39;, \u0026#39;HD\u0026#39;, \u0026#39;Has Video\u0026#39;, \u0026#39;Loved\u0026#39;, \u0026#39;Matched\u0026#39;, \u0026#39;Music Video\u0026#39;, \u0026#39;Part Of Gapless Album\u0026#39;, \u0026#39;Playlist Only\u0026#39; ] for i, _ in enumerate(root[0][17]): temp_columns = list.copy(columns) if i % 2 == 1: for j, _ in enumerate(root[0][17][i]): if root[0][17][i][j].tag == \u0026#39;key\u0026#39;: if root[0][17][i][j].text in bool_columns: data[root[0][17][i][j].text].append(root[0][17][i][j+1].tag) temp_columns.remove(root[0][17][i][j].text) else: data[root[0][17][i][j].text].append(root[0][17][i][j+1].text) temp_columns.remove(root[0][17][i][j].text) for column in temp_columns: data[column].append(np.nan) # Every feature has the same amount of elements for k, v in data.items(): print(k, len(v)) Track ID 6724 Name 6724 Artist 6724 Album Artist 6724 Album 6724 Genre 6724 Kind 6724 Size 6724 Total Time 6724 Disc Number 6724 Disc Count 6724 Track Number 6724 Track Count 6724 Year 6724 Date Modified 6724 Date Added 6724 Bit Rate 6724 Sample Rate 6724 Play Count 6724 Play Date 6724 Play Date UTC 6724 Skip Count 6724 Skip Date 6724 Release Date 6724 Loved 6724 Album Loved 6724 Artwork Count 6724 Sort Album 6724 Sort Artist 6724 Sort Name 6724 Persistent ID 6724 Track Type 6724 Apple Music 6724 Clean 6724 Comments 6724 Compilation 6724 Composer 6724 Explicit 6724 Grouping 6724 HD 6724 Has Video 6724 Matched 6724 Movement Count 6724 Movement Name 6724 Movement Number 6724 Music Video 6724 Part Of Gapless Album 6724 Playlist Only 6724 Sort Album Artist 6724 Sort Composer 6724 Work 6724  Creating pandas DataFrame object using data df = pd.DataFrame(data) Converting column data types # Converting numeric columns numeric_columns = [ \u0026#39;Artwork Count\u0026#39;, \u0026#39;Bit Rate\u0026#39;, \u0026#39;Disc Count\u0026#39;, \u0026#39;Disc Number\u0026#39;, \u0026#39;Movement Count\u0026#39;, \u0026#39;Movement Number\u0026#39;, \u0026#39;Play Count\u0026#39;, \u0026#39;Play Date\u0026#39;, \u0026#39;Sample Rate\u0026#39;, \u0026#39;Size\u0026#39;, \u0026#39;Skip Count\u0026#39;, \u0026#39;Track Count\u0026#39;, \u0026#39;Track ID\u0026#39;, \u0026#39;Track Number\u0026#39;, \u0026#39;Year\u0026#39; ] df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric) # Converting date columns date_columns = [ \u0026#39;Date Added\u0026#39;, \u0026#39;Date Modified\u0026#39;, \u0026#39;Play Date UTC\u0026#39;, \u0026#39;Release Date\u0026#39;, \u0026#39;Skip Date\u0026#39; ] df[date_columns] = df[date_columns].apply(pd.to_datetime) # http://joabj.com/Writing/Tech/Tuts/Java/iTunes-PlayDate.html # Apparently Apple uses a different Unix epoch time df[\u0026#39;Play Date\u0026#39;] = pd.to_datetime(df[\u0026#39;Play Date\u0026#39;] - 2082826800, unit=\u0026#39;s\u0026#39;) # \u0026#39;Total Time\u0026#39; column values need to be converted to Timedelta values df[\u0026#39;Total Time\u0026#39;] = df[\u0026#39;Total Time\u0026#39;].apply(pd.to_numeric) df[\u0026#39;Total Time\u0026#39;] = pd.to_timedelta(df[\u0026#39;Total Time\u0026#39;], unit=\u0026#39;ms\u0026#39;) iTunes Library Analysis Because I have some track information in Korean and Japanese, I will have to set a custom font for matplotlib.\nChinese, Japanese, and Korean texts will not show in matplotlib plots properly unless a compatible font is used.\n# Setting matplotlib custom font font_location = \u0026#39;C:/Windows/Fonts/gulim.ttc\u0026#39; font_name = fm.FontProperties(fname=font_location).get_name() matplotlib.rc(\u0026#39;font\u0026#39;, family=font_name) Top 10 Most Played Songs - Play Count \u0026amp; Skip Count ax = df[[\u0026#39;Name\u0026#39;,\u0026#39;Play Count\u0026#39;, \u0026#39;Skip Count\u0026#39;]] \\ .sort_values(by=\u0026#39;Play Count\u0026#39;, ascending=False) \\ .head(10) \\ .plot(kind=\u0026#39;barh\u0026#39;, x=\u0026#39;Name\u0026#39;, y=[\u0026#39;Play Count\u0026#39;, \u0026#39;Skip Count\u0026#39;]) ax.invert_yaxis() \r\r\rTop 10 Most Skipped Songs - Play Count \u0026amp; Skip Count ax = df[[\u0026#39;Name\u0026#39;,\u0026#39;Play Count\u0026#39;, \u0026#39;Skip Count\u0026#39;]] \\ .sort_values(by=\u0026#39;Skip Count\u0026#39;, ascending=False) \\ .head(10) \\ .plot(kind=\u0026#39;barh\u0026#39;, x=\u0026#39;Name\u0026#39;, y=[\u0026#39;Play Count\u0026#39;, \u0026#39;Skip Count\u0026#39;]) ax.invert_yaxis() \r\r\rTop 5 Artists by Play Count ax = df.groupby(by=\u0026#39;Artist\u0026#39;).sum()[\u0026#39;Play Count\u0026#39;] \\ .sort_values(ascending=False)[:5] \\ .plot(kind=\u0026#39;barh\u0026#39;) ax.invert_yaxis() \r\r\rTop 10 Genres by Track Count ax = df[\u0026#39;Genre\u0026#39;].value_counts()[0:10].plot(kind=\u0026#39;barh\u0026#39;) ax.invert_yaxis() \r\r\rTop 10 Genres by Play Count ax = df.groupby(by=\u0026#39;Genre\u0026#39;).sum() \\ .sort_values(by=\u0026#39;Play Count\u0026#39;, ascending=False)[\u0026#39;Play Count\u0026#39;][0:10] \\ .plot(kind=\u0026#39;barh\u0026#39;); ax.invert_yaxis() \r\r\rConverting \u0026ldquo;Total Time\u0026rdquo; column from Timedelta values to float values print(\u0026#39;Method #1 : seconds\u0026#39;) print(df[\u0026#39;Total Time\u0026#39;].astype(\u0026#39;timedelta64[s]\u0026#39;)[0:3], \u0026#39;\\n\u0026#39;) print(\u0026#39;Method #1 : minutes\u0026#39;) print(df[\u0026#39;Total Time\u0026#39;].astype(\u0026#39;timedelta64[m]\u0026#39;)[0:3], \u0026#39;\\n\u0026#39;) print(\u0026#39;Method #2 : minutes\u0026#39;) print(df[\u0026#39;Total Time\u0026#39;][0:3]/pd.Timedelta(minutes=1), \u0026#39;\\n\u0026#39;) print(\u0026#39;Method #2 : number of 1 minute 30 seconds in each song duration\u0026#39;) print(df[\u0026#39;Total Time\u0026#39;][0:3]/pd.Timedelta(minutes=1,seconds=30)) Method #1 : seconds 0 218.0 1 203.0 2 218.0 Name: Total Time, dtype: float64 Method #1 : minutes 0 3.0 1 3.0 2 3.0 Name: Total Time, dtype: float64 Method #2 : minutes 0 3.638083 1 3.388383 2 3.649900 Name: Total Time, dtype: float64 Method #2 : number of 1 minute 30 seconds in each song duration 0 2.425389 1 2.258922 2 2.433267 Name: Total Time, dtype: float64  Distribution of \u0026ldquo;Total Time\u0026rdquo; (df[\u0026#39;Total Time\u0026#39;]/pd.Timedelta(minutes=1)) \\ .plot.hist(bins=np.arange(0,10.5,0.5), edgecolor=\u0026#39;black\u0026#39;) plt.xlabel(\u0026#39;Minutes\u0026#39;); \r\r\rDistribution of \u0026ldquo;Total Time\u0026rdquo; for \u0026ldquo;Loved\u0026rdquo; Songs (df.loc[df[\u0026#39;Loved\u0026#39;] == \u0026#39;true\u0026#39;][\u0026#39;Total Time\u0026#39;]/pd.Timedelta(minutes=1)) \\ .plot.hist(bins=np.arange(0,10.5,0.5),edgecolor=\u0026#39;black\u0026#39;); \r\r\r","permalink":"https://junschoi.github.io/posts/itunes_library_analysis/","tags":["xml","defaultdict","matplotlib"],"title":"iTunes Library Data Analysis using Python"},{"categories":["Python"],"contents":"This is problem 698 on leetcode. The solution function below is very similar to the one that is provided by leetcode but I made some changes to the solutions function so that it also returns the subsets when it\u0026rsquo;s possible to divide the array into k subsets.\nProblem Statement\nGiven an array of integers nums and a positive integer k, find whether it\u0026rsquo;s possible to divide this array into k non-empty subsets whose sums are all equal.\nTwo main keys to solve this problem:\n Use recursion to see if all integers from input array can be placed into k partitions without going over the target value. Use backtracking method when adding an integer from input array goes over the target value and all partitions are filled out.  Also, there are some optimizations that we can make to speed up the process or rule out certain cases\n Sort the array and place the largest numbers to a partition first. If sum of all numbers divided by k equals to decimal value, return false. If largest value in array is greater than target value, return false. If largest value in array is equal to target value, place the value in an empty partition.  def canPartitionKSubsets(nums, k): # Optimization #1 nums.sort() partitions = [[] for i in range(k)] target, rem = divmod(sum(nums), k) # Optimization #2 if rem: return False # Optimization #3 if nums[-1] \u0026gt; target: return False # Optimization #4 while nums and nums[-1] == target: v = nums.pop() k -= 1 partitions[k].append(v) # Recursive function def search(partitions): if not nums: return True, partitions v = nums.pop() for i in range(len(partitions)): if sum(partitions[i]) + v \u0026lt;= target: partitions[i].append(v) if search(partitions): return True, partitions partitions[i].pop() if not partitions[i]: break nums.append(v) return False return search(partitions) print(canPartitionKSubsets([0, 0], 3)) print(canPartitionKSubsets([312, 310, 268, 268, 241, 239, 196, 128, 95, 81], 2)) print(canPartitionKSubsets([7, 3, 5, 12, 2, 1, 5, 3, 8, 4, 6, 4], 2)) print(canPartitionKSubsets([7, 3, 5, 12, 2, 1, 5, 3, 8, 4, 6, 4], 3)) print(canPartitionKSubsets([7, 3, 5, 12, 2, 1, 5, 3, 8, 4, 6, 4], 4)) print(canPartitionKSubsets([7, 3, 5, 12, 2, 1, 5, 3, 8, 4, 6, 4], 5)) print(canPartitionKSubsets([7, 3, 5, 12, 2, 1, 5, 3, 8, 4, 6, 4], 6)) print(canPartitionKSubsets([2, 4, 6, 2], 2)) (True, [[], [0], [0]]) (True, [[312, 241, 239, 196, 81], [310, 268, 268, 128, 95]]) (True, [[12, 8, 7, 3], [6, 5, 5, 4, 4, 3, 2, 1]]) (True, [[12, 8], [7, 6, 5, 2], [5, 4, 4, 3, 3, 1]]) (True, [[12, 3], [8, 7], [6, 5, 4], [5, 4, 3, 2, 1]]) (True, [[8, 4], [7, 5], [6, 5, 1], [4, 3, 3, 2], [12]]) False False  ","permalink":"https://junschoi.github.io/posts/partition_k_equal_subsets/","tags":["leetcode","recursion"],"title":"Partition to K Equal Sum Subsets"},{"categories":["Python"],"contents":"There are many python packages that can solve linear programming problems. In this demo, I will be using scipy\u0026rsquo;s linprog method.\nRead more here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linprog.html\nExample 1 A farmer has 240 acres of land. An acre of corn will yield 40 USD in profit while an acre of oats will yield 30 USD in profit. He has 320 hours available to work on his farm. An acre of corn requires 2 hours of labor while an acre of oats requres only 1 hour of labor. How many acres of each should be planted to maximize profits?\nLet x = acres of corn and y = acres of oat.\nObjective function:\n$$ 40x + 30y $$\nLand available:\n$$ x + y ≤ 240 $$\nHours available:\n$$ 2x + y ≤ 320 $$\nfrom scipy.optimize import linprog import numpy as np var_list = [\u0026#39;corn\u0026#39;,\u0026#39;oat\u0026#39;] # X matrix A_ineq = [[2,1],[1,1]] # Inequality equations, LHS B_ineq = [[320,240]] # Inequality equations, RHS # Equality equations are left blank since there is none A_eq = [[]] # Equality equations, LHS B_eq = [[]] # Equality equations, RHS # Objective function coefficients are negative because # linprog method by default minimizes the objective function and # max(f(x)) = -min(-f(x)) obj_fun = [-40,-30] # Objective function res_no_bounds = linprog(c=obj_fun, A_ub=A_ineq, b_ub=B_ineq, method=\u0026#39;simplex\u0026#39;) print(res_no_bounds) # Optimization result shown in \u0026#39;x\u0026#39; # Farmer should plant 80 acres of corn and 160 acres of oat  con: array([], dtype=float64) fun: -8000.0 message: 'Optimization terminated successfully.' nit: 3 slack: array([0., 0.]) status: 0 success: True x: array([ 80., 160.])  Example 2 A concrete company transports concrete from three plants 1, 2, and 3, to three contruction sites, A, B, and C.\nThe plants are able to supply the following numbers of tons per week:\n   Plant Supply     1 100   2 300   3 300     The requirements of the sites, in number of tons per week are:\n   Site Demand     A 300   B 200   C 200    Since sum of demand and sum of supply are equal, this is a balanced problem.\nThe cost of transporting 1 ton of concrete from each plant to each site is shown in the figure below.\n    A B C Supply     1 5 4 3 100   2 8 4 3 300   3 9 7 5 300   Demand 300 200 200 -     Objective Function:\n$$ 5x_{1A} + 4x_{1B} + 3x_{1C} + 8x_{2A} + 4x_{2B} + 3x_{2C} + 9x_{3A} + 7x_{3B} + 5x_{3c} $$\n Supply Contraints:\n$$ x_{1A} + x_{1B} + x_{1C} = 100 $$\n$$ x_{2A} + x_{2B} + x_{2C} = 300 $$\n$$ x_{3A} + x_{3B} + x_{3C} = 300 $$\n Demand Contraints:\n$$ x_{1A} + x_{2A} + x_{3A} = 300 $$\n$$ x_{1B} + x_{2B} + x_{3B} = 200 $$\n$$ x_{1C} + x_{2C} + x_{3C} = 200 $$\n# Declaring X matrix variables plants = \u0026#39;123\u0026#39; sites = \u0026#39;abc\u0026#39; var_list = [\u0026#39;x\u0026#39;+p+s for p in plants for s in sites] # X matrix var_list ['x1a', 'x1b', 'x1c', 'x2a', 'x2b', 'x2c', 'x3a', 'x3b', 'x3c']  # Inequality equations are left blank since there is none A_ineq = [] # Inequality equations, LHS B_ineq = [] # Inequality equations, RHS A_eq = [ [1,1,1,0,0,0,0,0,0], [0,0,0,1,1,1,0,0,0], [0,0,0,0,0,0,1,1,1], [1,0,0,1,0,0,1,0,0], [0,1,0,0,1,0,0,1,0], [0,0,1,0,0,1,0,0,1] ] # Equality equations, LHS B_eq = [[100,300,300,300,200,200]] # Equality equations, RHS obj_fun = [5,4,3,8,4,3,9,7,5] # Objective function res_no_bounds = linprog(c=obj_fun, A_eq=A_eq, b_eq=B_eq, method=\u0026#39;simplex\u0026#39;) print(res_no_bounds)  con: array([0., 0., 0., 0., 0., 0.]) fun: 3900.0 message: 'Optimization terminated successfully.' nit: 7 slack: array([], dtype=float64) status: 0 success: True x: array([100., 0., 0., 0., 200., 100., 200., 0., 100.]) C:\\Users\\junsc\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_linprog_util.py:763: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints. warn(redundancy_warning, OptimizeWarning)  Optimization result in table form:\n    A B C Supply     1 100 0 0 100   2 0 200 100 300   3 200 0 100 300   Demand 300 200 200 -    You may notice below warning:\nOptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints. warn(redundancy_warning, OptimizeWarning) To check which constraints are redundant, find the reduced row echelon form of A_eq matrix to see which equations are redundant constraints.\n# Using synmpy to find reduced row echelon form of A_eq import sympy A_eq_check = sympy.Matrix(A_eq) A_eq_check.rref() (Matrix([ [1, 0, 0, 0, -1, -1, 0, -1, -1], [0, 1, 0, 0, 1, 0, 0, 1, 0], [0, 0, 1, 0, 0, 1, 0, 0, 1], [0, 0, 0, 1, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0]]), (0, 1, 2, 3, 6))  The last equation is the redundant constraint.\nThe redundancy warning message will not show after running the same code without this redundant constraint, and the result will remain the same.\nA_eq = [ [1,1,1,0,0,0,0,0,0], [0,0,0,1,1,1,0,0,0], [0,0,0,0,0,0,1,1,1], [1,0,0,1,0,0,1,0,0], [0,1,0,0,1,0,0,1,0] #[0,0,1,0,0,1,0,0,1] Redundant constraint removed ] # Equality equations, LHS B_eq = [[100,300,300,300,200]] # RHS for redundant constraint removed obj_fun = [5,4,3,8,4,3,9,7,5] # Objective function res_no_bounds = linprog(c=obj_fun, A_eq=A_eq, b_eq=B_eq, method=\u0026#39;simplex\u0026#39;) print(res_no_bounds)  con: array([0., 0., 0., 0., 0.]) fun: 3900.0 message: 'Optimization terminated successfully.' nit: 9 slack: array([], dtype=float64) status: 0 success: True x: array([100., 0., 0., 0., 200., 100., 200., 0., 100.])  ","permalink":"https://junschoi.github.io/posts/linear_programming_demo/","tags":["scipy","linprog","sympy"],"title":"Linear Programming Demo"},{"categories":null,"contents":"👋 Hi, welcome to my blog!\nMy name is Jun Choi, and I\u0026rsquo;m a self-taught Python programmer and an aspiring data scientist with a special interest in automation. Before I committed myself to learning Python and data science, I used to manage Amazon third party seller\u0026rsquo;s warehouse and work as a freight forwarder, coordinating both import and export shipments.\nMy main programming language is Python. I decided to learn Python around late 2019 because I was looking for ways to automate my tasks, and Python seemed to be the best option from my research. I\u0026rsquo;ve had a little bit of coding experience before with Java and Matlab back in high school and university, but they never really interested me back then. However, after watching Code Academy\u0026rsquo;s Intro to Python video on Youtube, I was immediately hooked. I searched for more resources online and found Al Sweigar\u0026rsquo;s Automate the Boring Stuff. After that, I began working on small Python scripts and personal projects which automated a lot of my daily tasks. While I was happy that I was able to make these helpful scripts, I wanted to learn more, so I decided to quit my day job to learn Python and data science full time.\nOther than technical things, I like competitive PC games, photography, music, movies, and soccer (specifically, the English Premier League).\nI will be using this blog to post anything related to programming, projects, photography, technology, travels, etc.\nThank you for visiting my blog and hope you enjoy reading my posts!\n","permalink":"https://junschoi.github.io/about/","tags":null,"title":"About Me"}]